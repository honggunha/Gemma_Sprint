{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mq8MJVLpfBZr"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEezFeHYytr2"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install bitsandbytes\n",
        "!pip install peft\n",
        "!pip install trl\n",
        "!pip install accelerate\n",
        "!pip install llmcompressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1aTD0mEkowu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, TrainingArguments\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer\n",
        "from llmcompressor.transformers import SparseAutoModelForCausalLM, oneshot\n",
        "from llmcompressor.modifiers.quantization import GPTQModifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qf5yYPsJi03M"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"llama-duo/gemma2b-coding-eval-by-claude3sonnet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABkdG7LZ1Ixg"
      },
      "outputs": [],
      "source": [
        "dataset['gemma2b_coding_gpt4o_100k_by_claude3sonnet'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lCcJfxw9kMO"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(example):\n",
        "    prompt_list = []\n",
        "    for i in range(len(example['instructions'])):\n",
        "        prompt_list.append(r\"\"\"<bos><start_of_turn>user\n",
        "다음 명령에 따라 코드를 작성해 주세요:\n",
        "\n",
        "{}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "{}<end_of_turn><eos>\"\"\".format(example['instructions'][i], example['target_responses'][i]))\n",
        "    return prompt_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnOiSVus9lkI"
      },
      "outputs": [],
      "source": [
        "train_data = dataset['gemma2b_coding_gpt4o_100k_by_claude3sonnet']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAokDpoj9nwI"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=6,\n",
        "    lora_alpha = 8,\n",
        "    lora_dropout = 0.05,\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWNJhPC_92HX"
      },
      "outputs": [],
      "source": [
        "BASE_MODEL = \"google/gemma-2-2b-it\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\", quantization_config=bnb_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "tokenizer.padding_side = 'right'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtftbkVtAeNW"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    max_seq_length=512,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"outputs\",\n",
        "#        num_train_epochs = 1,\n",
        "        max_steps=300,\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=8,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        warmup_steps=90,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=100,\n",
        "        push_to_hub=False,\n",
        "        report_to='none',\n",
        "    ),\n",
        "    peft_config=lora_config,\n",
        "    formatting_func=generate_prompt,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGhLd-93dFue"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvdPvoyfAlXg"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "205mqAigA93Y"
      },
      "outputs": [],
      "source": [
        "ADAPTER_MODEL = \"lora_adapter\"\n",
        "\n",
        "trainer.model.save_pretrained(ADAPTER_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIyPuPiiDAhp"
      },
      "outputs": [],
      "source": [
        "!ls -alh lora_adapter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "id": "83mZFJGrFGus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I56vgH3qC_X3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from peft import PeftModel # Import PeftModel from peft\n",
        "\n",
        "#from huggingface_hub import login\n",
        "\n",
        "# Hugging Face에 로그인\n",
        "#login(token=\"your_huggingface_token\")\n",
        "\n",
        "# 모델을 불러오고 준비\n",
        "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map='auto', torch_dtype=torch.float16)\n",
        "model = PeftModel.from_pretrained(model, ADAPTER_MODEL, device_map='auto', torch_dtype=torch.float16)\n",
        "\n",
        "# 모델 병합 및 언로드\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# 로컬에 모델 저장\n",
        "model.save_pretrained('gemma-2-2b-it-coding')\n",
        "\n",
        "# 모델을 허브에 푸시\n",
        "model.push_to_hub(\"gemma-2-2b-it-coding\", commit_message=\"Initial commit of merged model for IT coding tasks.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}